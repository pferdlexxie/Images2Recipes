{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1. Upload dataset and generate embeddings"
      ],
      "metadata": {
        "id": "PhH_gzEJjF7U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADh-CGWuhivA"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "dataset = load_dataset(\"rogozinushka/povarenok-recipes\", download_mode=\"force_redownload\")\n",
        "df = pd.DataFrame(dataset['train'])\n",
        "def extract_keys(dict_str):\n",
        "    try:\n",
        "        ingredients = ast.literal_eval(dict_str)\n",
        "        keys = [key.lower() for key in ingredients.keys()]\n",
        "        return keys\n",
        "    except:\n",
        "        return []\n",
        "df['ingredient_names'] = df['ingredients'].apply(extract_keys)\n",
        "df = df.drop(columns=['ingredients'])"
      ],
      "metadata": {
        "id": "hgHsEAUQhntr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "\n",
        "# Load pre-trained RuBERT model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased\")\n",
        "model = AutoModel.from_pretrained(\"DeepPavlov/rubert-base-cased\")\n",
        "\n",
        "def get_rubert_embedding(word_list, max_length=128):\n",
        "    inputs = tokenizer(word_list, return_tensors='pt', is_split_into_words=True, padding=True, truncation=True, max_length=max_length)\n",
        "    outputs = model(**inputs)\n",
        "    cls_embedding = outputs.last_hidden_state[:, 0, :].squeeze().detach().numpy()\n",
        "    return cls_embedding\n",
        "\n",
        "df['ingredient_embedding'] = df['ingredient_names'].progress_apply(lambda x: get_rubert_embedding(x))\n",
        "\n",
        "df.to_csv('recipes_with_embeddings.csv', index=False)"
      ],
      "metadata": {
        "id": "TqPRRYXnhx0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2. Slice the 50k sample from original dataset"
      ],
      "metadata": {
        "id": "PEMcKgwZjWGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(df['ingredient_embedding'][0]))"
      ],
      "metadata": {
        "id": "oNXS63bMkD3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert embeddings to arrays"
      ],
      "metadata": {
        "id": "fue1Q0c1lw85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import re\n",
        "def convert_embedding(embedding_str):\n",
        "    cleaned_str = re.sub(r'\\s+', ' ', embedding_str.replace('[', '').replace(']', '').strip())\n",
        "    embedding_array = np.fromstring(cleaned_str, sep=' ')\n",
        "    return embedding_array\n",
        "\n",
        "df['ingredient_embedding'] = df['ingredient_embedding'].apply(convert_embedding)\n",
        "print(df['ingredient_embedding'].head())"
      ],
      "metadata": {
        "id": "rWiosrxDjwI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use stratified sampling to preserve variance"
      ],
      "metadata": {
        "id": "KEzwhxI2l2XQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['embedding_mean'] = df['ingredient_embedding'].apply(np.mean)\n",
        "df['embedding_bin'] = pd.qcut(df['embedding_mean'], q=50, duplicates='drop')\n",
        "\n",
        "sampling_fraction = 50000 / len(df)\n",
        "sample = df.groupby('embedding_bin', group_keys=False).apply(lambda x: x.sample(frac=sampling_fraction, random_state=42))\n",
        "\n",
        "print(sample.head())\n",
        "print(sample.shape)"
      ],
      "metadata": {
        "id": "n3VTm99JkI9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_variances = np.var(np.vstack(df['ingredient_embedding']), axis=0)\n",
        "sampled_variances = np.var(np.vstack(sample['ingredient_embedding']), axis=0)\n",
        "mean_original_variances = np.mean(original_variances)\n",
        "mean_sampled_variances = np.mean(sampled_variances)\n",
        "\n",
        "print(\"Mean variance of the original dataset:\", mean_original_variances)\n",
        "print(\"Mean variance of the sampled dataset:\", mean_sampled_variances)\n",
        "\n",
        "sample = sample.drop(['embedding_mean', 'embedding_bin'], axis = 1)\n",
        "sample.to_csv('sampled_df.csv', index=False)"
      ],
      "metadata": {
        "id": "7bsO487kkSP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3. Shrink the embeddings"
      ],
      "metadata": {
        "id": "Xpbx27fMlRRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_size = len(sample['ingredient_embedding'].iloc[0])\n",
        "print(embedding_size)"
      ],
      "metadata": {
        "id": "wBCdjrDHm0rl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform PCA analysis to find the optimal dimensions"
      ],
      "metadata": {
        "id": "LpknIptdnWd-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "pooled_embeddings = np.stack(sample['ingredient_embedding'].values)\n",
        "\n",
        "pca = PCA(n_components=None)\n",
        "pca.fit(pooled_embeddings)\n",
        "\n",
        "eigen_vals = pca.explained_variance_\n",
        "\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.plot(eigen_vals, marker='o', color='blue')\n",
        "plt.xlabel(\"Eigenvalue number\")\n",
        "plt.ylabel(\"Eigenvalue size\")\n",
        "plt.title(\"Scree Plot\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xnYeZNQamie_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exp_var = pca.explained_variance_ratio_  * 100\n",
        "cum_exp_var = np.cumsum(exp_var)\n",
        "\n",
        "a = 769\n",
        "\n",
        "plt.bar(range(1, a), exp_var, align='center',\n",
        "        label='Individual explained variance')\n",
        "\n",
        "plt.step(range(1, a), cum_exp_var, where='mid',\n",
        "         label='Cumulative explained variance', color='red')\n",
        "\n",
        "plt.ylabel('Explained variance percentage')\n",
        "plt.xlabel('Principal component index')\n",
        "plt.xticks(ticks=list(range(1, a)))\n",
        "plt.legend(loc='best')\n",
        "plt.tight_layout()\n",
        "\n",
        "nc = np.argmax(cum_exp_var >= 85) + 1\n",
        "print(f\"Dimensions to keep 85% variance: {nc}\")"
      ],
      "metadata": {
        "id": "6iyWRWfxmm-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shrink embeddings to 70 dimensions per vector"
      ],
      "metadata": {
        "id": "7weQxoQ2nyEb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "pca = PCA(n_components=70)\n",
        "reduced_embeddings = pca.fit_transform(pooled_embeddings)\n",
        "\n",
        "joblib_file = \"pca_model.pkl\"\n",
        "joblib.dump(pca, joblib_file)\n",
        "\n",
        "df_reduced = sample.drop(columns=['ingredient_embedding']).copy()\n",
        "df_reduced['ingredient_embedding'] = list(reduced_embeddings)\n",
        "\n",
        "df_reduced.to_csv('reduced_sample.csv', index=False)"
      ],
      "metadata": {
        "id": "j913r5hFmue7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_size = len(df_reduced['ingredient_embedding'].iloc[0])\n",
        "print(embedding_size)"
      ],
      "metadata": {
        "id": "DMjgqr1sob4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4. Upload to Qdrant database"
      ],
      "metadata": {
        "id": "TAmqzfxTn6nv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http import models\n",
        "\n",
        "api_key = 'hidden'\n",
        "url = 'hidden'\n",
        "client = QdrantClient(api_key=api_key, url=url)"
      ],
      "metadata": {
        "id": "4qZVvE3fn6W7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collection_name = \"recipes_reduced\"\n",
        "\n",
        "client.create_collection(\n",
        "    collection_name=collection_name,\n",
        "    vectors_config=models.VectorParams(size=70, distance=models.Distance.COSINE)\n",
        ")\n",
        "\n",
        "vectors = df_reduced['ingredient_embedding'].tolist()\n",
        "payload = df_reduced[['url', 'name', 'ingredient_names']].to_dict('records')\n",
        "\n",
        "client.upload_collection(\n",
        "    collection_name=collection_name,\n",
        "    vectors=vectors,\n",
        "    payload=payload,\n",
        "    ids=None\n",
        ")"
      ],
      "metadata": {
        "id": "LmqJydDEpHn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5. Test search process"
      ],
      "metadata": {
        "id": "iA5wlSevs1P-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From previous parts we have 2 lists of words, which we want to embed and use as search vectors in database"
      ],
      "metadata": {
        "id": "OvBaPJkruS6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import joblib\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased\")\n",
        "model = AutoModel.from_pretrained(\"DeepPavlov/rubert-base-cased\")\n",
        "\n",
        "\n",
        "raw_prediction = ['лимон', 'куриное филе', 'кефир']\n",
        "raw_synonyms = ['апельсин', 'гусь', 'йогурт']\n",
        "\n",
        "def get_rubert_embedding(word_list, max_length=128):\n",
        "    inputs = tokenizer(' '.join(word_list), return_tensors='pt', padding=True, truncation=True, max_length=max_length)\n",
        "    outputs = model(**inputs)\n",
        "    cls_embedding = outputs.last_hidden_state[:, 0, :].squeeze().detach().numpy()\n",
        "    return cls_embedding\n",
        "\n",
        "prediction_embedding = get_rubert_embedding(raw_prediction)\n",
        "synonyms_embedding = get_rubert_embedding(raw_synonyms)\n",
        "prediction_embedding = prediction_embedding.reshape(1, -1)\n",
        "synonyms_embedding = synonyms_embedding.reshape(1, -1)\n",
        "pca = joblib.load(\"pca_model.pkl\")\n",
        "\n",
        "prediction_embedding_70d = pca.transform(prediction_embedding)\n",
        "synonyms_embedding_70d = pca.transform(synonyms_embedding)\n",
        "\n",
        "prediction_embedding_70d = prediction_embedding_70d.reshape(70)\n",
        "synonyms_embedding_70d = synonyms_embedding_70d.reshape(70)"
      ],
      "metadata": {
        "id": "pQ5q3lPNtyL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FInd and display top-5 closest matches"
      ],
      "metadata": {
        "id": "bUIxNkvUuqH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_search(collection_name, query_vector, limit=5):\n",
        "    response = client.search(\n",
        "        collection_name=collection_name,\n",
        "        query_vector=query_vector,\n",
        "        limit=limit,\n",
        "        with_payload=True\n",
        "    )\n",
        "    return response\n",
        "\n",
        "collection_name_70 = \"recipes_reduced\"\n",
        "print(\"INITIAL\", raw_prediction)\n",
        "\n",
        "search_results_70 = perform_search(collection_name_70, prediction_embedding_70d )\n",
        "print(\"\\nSearch results for INITIAL :\")\n",
        "for result in search_results_70:\n",
        "    print(result.payload)\n",
        "\n",
        "print(\"\\n\\nSYNONYMS\", raw_synonyms)\n",
        "search_results_70 = perform_search(collection_name_70, synonyms_embedding_70d)\n",
        "print(\"\\nSearch results for SYNONYMS :\")\n",
        "for result in search_results_70:\n",
        "    print(result.payload)\n"
      ],
      "metadata": {
        "id": "G0GnwyhBt59Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}